{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some problems with this approach:\n",
    "# - Simpler vocab !=? simpler grammar\n",
    "# - May want simpler words to have a higher probability, \n",
    "#     rather than just removing all words.\n",
    "# Idea:\n",
    "# what if we blend the output probabilities\n",
    "# with the probabilities that arise in a corpus.\n",
    "# e.g. the probabilities that jk rowling uses, etc.\n",
    "# At that point might as well finetune, \n",
    "# but would be interesting to see what happens/compare/benchmark.\n",
    "# Idea:\n",
    "# Might be fun to do a grid search/random search, over .generate() parameters,\n",
    "# and compare the average perplexity (averaged over N generation attempts per parameter set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPTNeoForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = device(type='cuda')\n",
      "MODEL = 'EleutherAI/gpt-neo-1.3B'\n"
     ]
    }
   ],
   "source": [
    "if not globals().get(\"model\"):\n",
    "    SMALL_MODEL = False  # use smaller model for testing\n",
    "    MODEL = \"EleutherAI/gpt-neo-125M\" if SMALL_MODEL else \"EleutherAI/gpt-neo-1.3B\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GPTNeoForCausalLM.from_pretrained(MODEL).half().to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
    "    # Keep a reference to the original lm_head\n",
    "    original_lm_head = model.lm_head\n",
    "print(f\"{device = }\")\n",
    "print(f\"{MODEL = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original head: original_lm_head = Linear(in_features=2048, out_features=50257, bias=False)\n",
      "Masked head: model.lm_head = MaskedLMHead(\n",
      "  (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MaskedLMHead(torch.nn.Module):\n",
    "    \"\"\"This class is a wrapper around the language model head of the gpt neo model.\n",
    "    It is used to mask the logits of the language model head,\n",
    "    to restrict the vocabulary of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lm_head, mask):\n",
    "        super().__init__()\n",
    "        self.lm_head = lm_head\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        logits = self.lm_head(*args, **kwargs)\n",
    "        masked_logits = logits.masked_fill(self.mask, float(\"-inf\"))\n",
    "        return masked_logits\n",
    "\n",
    "\n",
    "# Create a mask that makes all tokens except \"ok\"\n",
    "# have a predicted probability of zero\n",
    "tokenizer_dict = tokenizer.get_vocab()\n",
    "mask = torch.ones(50257, device=device, dtype=torch.bool)\n",
    "idx_of_ok = tokenizer_dict[\"ok\"]\n",
    "mask[idx_of_ok] = 0  # 0 means don't mask\n",
    "\n",
    "# Replace the model's lm_head with our masked version\n",
    "print(f\"Original head: {original_lm_head = }\")\n",
    "model.lm_head = MaskedLMHead(original_lm_head, mask)\n",
    "print(f\"Masked head: {model.lm_head = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities = tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward0>)\n",
      "probabilities[..., idx_of_ok] = tensor([[1., 1., 1., 1., 1., 1., 1.]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run the model on a dummy input and\n",
    "# confirm that all tokens except for \"ok\"\n",
    "# get a probability of 0\n",
    "prompt = \"There are five pastries on the\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "attention_mask = torch.zeros_like(input_ids)\n",
    "logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "print(f\"{probabilities = }\")\n",
    "print(f\"{probabilities[..., idx_of_ok] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is going to be okokokokokokokokokok\n"
     ]
    }
   ],
   "source": [
    "# See what happens when we try to generate text,\n",
    "# using our model with the masked language model head\n",
    "prompt = \"Everything is going to be \"\n",
    "tokens = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "gen_tokens = model.generate(\n",
    "    **tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_new_tokens=10,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary from vocabulary.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429/429 [00:00<00:00, 13271.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a constrained vocabulary,\n",
    "# by using only the words (/tokens) found in our corpus dir.\n",
    "# (We'll load it from a file if it exists, otherwise we'll create it.)\n",
    "\n",
    "CORPUS_DIR = \"vocab-corpus\" #\"/media/sid/bigdata/datasets/harry-potter-text\"\n",
    "VOCAB_PATH = \"vocabulary.json\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def create_vocabulary(corpus_dir):\n",
    "    corpus_files = pathlib.Path(CORPUS_DIR).glob(\"*.txt\")\n",
    "    words = set()\n",
    "    for i, corpus_file in enumerate(corpus_files):\n",
    "        lines = re.sub(r\"\\n+\", \"\\n\", corpus_file.read_text()).split(\"\\n\")\n",
    "        # Process the text 200 lines at a time,\n",
    "        # to avoid memory issues.\n",
    "        for i in tqdm(range(0, len(lines), 200), desc=f\"File {i}\"):\n",
    "            text = \"\\n\".join(lines[i : i + 200])\n",
    "            words.update([token.text for token in nlp(text)])\n",
    "    words = set(word.strip(\" \") for word in words)\n",
    "    # Make sure each word has a the capitalisation: word, Word\n",
    "    for word in words.copy():\n",
    "        words.update([word.lower(), word.capitalize()])\n",
    "    # Include each word with spaces around it\n",
    "    for word in words.copy():\n",
    "        words.update([f\" {word} \", f\" {word}\", f\"{word} \"])\n",
    "    words.update([\" \"])\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "if not pathlib.Path(VOCAB_PATH).exists():\n",
    "    print(\"Creating vocabulary...\")\n",
    "    vocabulary = create_vocabulary(CORPUS_DIR)\n",
    "    with open(VOCAB_PATH, \"w\") as f:\n",
    "        json.dump(vocabulary, f, indent=2)\n",
    "else:\n",
    "    with open(VOCAB_PATH) as f:\n",
    "        vocabulary = json.load(f)\n",
    "    print(f\"Loaded vocabulary from {VOCAB_PATH}.\")\n",
    "\n",
    "\n",
    "# Tokenize this vocabulary with our tokenizer\n",
    "allowed_token_indices = set()\n",
    "for word in tqdm(vocabulary):\n",
    "    token_indices = tokenizer.encode(word, add_special_tokens=False)\n",
    "    allowed_token_indices.update(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:21<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a mask that makes all tokens except\n",
    "# the ones in our vocabulary have a predicted probability of zero\n",
    "mask = torch.ones(50257, device=device, dtype=torch.bool)\n",
    "mask[list(allowed_token_indices)] = 0  # 0 means don't mask\n",
    "# Use this in our MaskedLMHead for the language model\n",
    "model.lm_head = MaskedLMHead(original_lm_head, mask)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Little Jane ran up the lane\n",
    "To hang her clothes a-drying;\n",
    "She called for Nell to ring the bell,\n",
    "For Jack and Jill were dying.\n",
    "Nimble Dick ran up so quick,\n",
    "He tumbled over a timber,\n",
    "And bent his bow to shoot a crow,\n",
    "And killed a cat in the window. \n",
    "\"\"\".strip()\n",
    "input_encodings = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "# See what happens when we try to generate text,\n",
    "# using our model with the masked language model head\n",
    "generated_texts = []\n",
    "for i in tqdm(range(40)):\n",
    "    torch.manual_seed(i)\n",
    "    np.random.seed(i)\n",
    "    gen_tokens = model.generate(\n",
    "        **input_encodings,\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        # temperature=0.9,\n",
    "        # num_beams=5,\n",
    "        # typical_p=0.7,\n",
    "        repetition_penalty=10.0,\n",
    "    )\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    generated_texts.append(gen_text)\n",
    "    # seperate the prompt from the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 36.85it/s]\n",
      "100%|██████████| 97/97 [00:02<00:00, 36.31it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 35.97it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.43it/s]\n",
      "100%|██████████| 97/97 [00:02<00:00, 36.53it/s]\n",
      "100%|██████████| 96/96 [00:02<00:00, 36.55it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.38it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.61it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.46it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.09it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.61it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.49it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.47it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.22it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.49it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.89it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.54it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 37.03it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.53it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.23it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.10it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.00it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.71it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.65it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.32it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.32it/s]\n",
      "100%|██████████| 97/97 [00:02<00:00, 36.87it/s]\n",
      "100%|██████████| 97/97 [00:02<00:00, 36.45it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.70it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.83it/s]\n",
      "100%|██████████| 97/97 [00:02<00:00, 36.15it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.11it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 35.99it/s]\n",
      "100%|██████████| 94/94 [00:02<00:00, 37.05it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.61it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.56it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 36.04it/s]\n",
      "100%|██████████| 98/98 [00:02<00:00, 36.53it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.24it/s]\n",
      "100%|██████████| 99/99 [00:02<00:00, 36.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexities = [22.796335038836506, 64.54056506488412, 1.2567036303927295, 249.41861558356214, 59.85650366122411, 142.55414367216824, 28.50182903034601, 148.45406628006202, 87.27959976708215, 40.577639982269325, 50.214647283547016, 154.0925203251954, 2.0459603939989828, 78.00486840841882, 42.76320028940462, 54.6839393571641, 44.50173344079129, 20.49421244108672, 69.71377051421017, 70.67250233269793, 2.49397815788712, 57.81493442348986, 85.8760179147562, 68.99754698737738, 85.80091209609346, 106.38654954769524, 49.960863951113915, 44.962026701591256, 70.26894960386151, 79.13663857308812, 141.22372212342958, 15.678126094907935, 24.71580520660862, 218.66673480676752, 117.55381838115538, 136.6527587061457, 3.6199115251073994, 79.89495688750674, 41.882079596977306, 60.22100597449658]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute perplexity, to estimate the quality of the results.\n",
    "# Restore the models original lm_head for this\n",
    "# references: \n",
    "# https://stackoverflow.com/a/61990477\n",
    "# https://huggingface.co/docs/transformers/perplexity\n",
    "model.lm_head = original_lm_head\n",
    "start_loc = input_encodings.input_ids.shape[-1]\n",
    "perplexities = []\n",
    "for generated_text in generated_texts:\n",
    "    encodings = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
    "    perplexity_values = []\n",
    "    for i in tqdm(range(start_loc, encodings.input_ids.shape[-1])):\n",
    "        input_ids = encodings.input_ids[:, :i]\n",
    "        label = encodings.input_ids[:, i]\n",
    "        with torch.no_grad():\n",
    "            # next token prediction\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "            p_next = logits[0, -1, :].softmax(0)[label]\n",
    "            # perplexity\n",
    "            perplexity = torch.log(p_next)\n",
    "            perplexity_values.append(perplexity.item())\n",
    "    perplexities.append(np.exp(-np.mean(perplexity_values)))\n",
    "print(f\"{perplexities = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump results to a text file\n",
    "with open(\"results.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "\"\"\".join(f\"{perplexity}\\n{result}\" for perplexity, result in sorted(zip(perplexities, generated_texts))))\n",
    "\n",
    "# Hmm, seems like perplexity isn't that useful, \n",
    "# the results with low perplexity seem to often have\n",
    "# a lot of repetition, and the results with high perplexity\n",
    "# seem to be more unique, but can also lack coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little| Jane| ran| up| the| lane|\n",
      "|To| hang| her| clothes| a|-|d|rying|;|\n",
      "|She| called| for| N|ell| to| ring| the| bell|,|\n",
      "|For| Jack| and| Jill| were| dying|.|\n",
      "|N|imble| Dick| ran| up| so| quick|,|\n",
      "|He| t|umbled| over| a| timber|,|\n",
      "|And| bent| his| bow| to| shoot| a| crow|,|\n",
      "|And| killed| a| cat| in| the| window|.|�|�|\n",
      "|\n",
      "|The| p|he|as|ent| of| j|in|p|ke|w| came| with| a| g|ail|er| after| the| g|ill|f|rot|,| as|\n",
      "|the| T|ame| could| see| he| did| she|w| The|s| F|f|her|,| In| the| Her|d| of| Gr|t|as|,|\n",
      "|M|m|end|,| For| he| went| to| the| p|he|as|ent| of| t|j|m|end|.|\n",
      "|\n",
      "|T|t|gr|t|as| got| down| to| t|j|t|and|g|to|of|,| To| see| the| her|d| of|\n",
      "|J|"
     ]
    }
   ],
   "source": [
    "# for debugging\n",
    "for id in gen_tokens[0].tolist():\n",
    "    print(tokenizer.decode(id), end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('reduced-vocab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aea90424739f68a8281285b5cf44775ab5652efc2cd8678b287940708131c264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
